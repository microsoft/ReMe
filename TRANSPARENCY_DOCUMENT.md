# Transparency Document for ReMe: A Personalized Cognitive Training Framework

## OVERVIEW

ReMe is a personalized cognitive training framework leveraging large language models. 

### WHAT CAN REME DO

ReMe was developed to support researchers, physicians and caregivers in creating AI chatbots that facilitate cognitive training. Based on this framework, training tasks based on large models and voice interaction can be quickly implemented, such as word guessing and life log recalling.
A detailed discussion of ReMe, including how it was developed and tested, can be found in our paper at: [[link]](https://arxiv.org/abs/2410.19733)

### INTENDED USES

ReMe is best suited for developing cognitive training Chatbot.
ReMe is being shared with the research community to develop cognitive training chatbots and design training plans that can validate the effectiveness of personalized cognitive training in various scenarios.
ReMe is intended to be used by domain experts of cognitive and behavioral science who are independently capable of evaluating the quality of outputs before acting on them.
	
### OUT-OF-SCOPE USES

ReMe is designed to help design and validate chatbot-based cognitive training methods, not suitable for direct treatment or patient care, especially by non-experts of cognitive and behavioral science.
We do not recommend using ReMe in commercial or real-world applications without further testing and development. It is being released for research purposes.

ReMe was not designed or evaluated for all possible downstream purposes. Developers should consider its inherent limitations (more below) as they select use cases, and evaluate and mitigate for accuracy, safety, and fairness concerns specific to each intended downstream use.
We do not recommend using ReMe in the context of high-risk decision making (e.g. in healthcare). 

ReMe does not provide medical or clinical opinions and is not designed to replace the role of qualified medical professionals in appropriately identifying, assessing, diagnosing, or managing medical conditions. 

## EVALUATION METHODS AND RESULTS

ReMe was evaluated based on its functional completeness and usability.
Functional completeness was verified through test cases and manual testing, confirming that the features such as chatbot's puzzle session initialization, voice input, model invocation, and voice output were all fully functional.

Usability was assessed via a user study, which evaluated the acceptance of the chatbot based cognitive training among the participants. Most users gave positive feedback on the system's fluency and considered the difficulty level to be moderate and relatively engaging. Specific evaluation results are described in the preprint on arXiv.

Groundedness was assessed using the auto-evaluator tool in Azure AI Foundry in 60 life scenario recollection puzzles cases, yielding an average score of 3.98. Subsequent manual verification of all responses confirmed that each output was either strictly grounded in factual accuracy or provided contextually appropriate puzzle responses, including hints, guidance, and analytical support. For developers, we strongly recommend implementing systematic groundedness testing after refining prompts and expanding puzzle variations to ensure response quality.
A detailed discussion of our evaluation methods and results can be found in our paper at: [[link](https://arxiv.org/abs/2410.19733)]

## LIMITATIONS

ReMe was developed for research and experimental purposes with usability assessment involving only participants without cognitive impairments Further testing and validation are needed before considering its application in commercial or real-world scenarios.

ReMe was designed and tested using the English and Chinese language. Performance in other languages may vary and should be assessed by someone who is both an expert in the expected outputs and a native speaker of that language.

Outputs generated by AI may include factual errors, fabrication, or speculation. Users are responsible for assessing the accuracy of generated content. All decisions leveraging outputs of the system should be made with human oversight and not be based solely on system outputs.

ReMe inherits any biases, errors, or omissions produced by its base model. Developers are advised to choose an appropriate base LLM/MLLM carefully, depending on the intended use case.

ReMe was mainly developed and tested on Azure OpenAI Service models (mainly GPT-4o). See [[link](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note?context=%2Fazure%2Fai-services%2Fopenai%2Fcontext%2Fcontext&tabs=text)] to understand the capabilities and limitations of this model. 

While ReMe accommodates the use of alternative LLMs, users are encouraged to perform extensive testing independently to confirm that these models fulfill their specific requirements for use.

ReMe does not provide medical or clinical opinions and is not designed to replace the role of qualified medical professionals in appropriately identifying, assessing diagnosing or managing medical conditions.

## BEST PRACTICES
Users can achieve the optimal cognitive training design by carefully reading the instructions/code and examples, thoroughly considering the design of the training plan, and conducting scientific validation.

We strongly encourage users to use LLMs/MLLMs that support robust Responsible AI mitigations, such as Azure Open AI (AOAI) services. Such services continually update their safety and RAI mitigations with the latest industry standards for responsible use. For more on AOAI’s best practices when employing foundations models for scripts and applications:
•	[Blog post on responsible AI features in AOAI that were presented at Ignite 2023](https://techcommunity.microsoft.com/t5/ai-azure-ai-services-blog/announcing-new-ai-safety-amp-responsible-ai-features-in-azure/ba-p/3983686)
•	[[Overview of Responsible AI practices for Azure OpenAI models]](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/overview) 
•	[Azure OpenAI Transparency Note](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/transparency-note)
•	[OpenAI’s Usage policies](https://openai.com/policies/usage-policies)
•	[Azure OpenAI’s Code of Conduct](https://learn.microsoft.com/en-us/legal/cognitive-services/openai/code-of-conduct)

Users are responsible for designing and conducting human-involved experiments legally and ethically. This could include securing appropriate copy rights, ensuring consent for participation in research and/or use of data, and/or the anonymization of data prior to use in research.   

Our demo code features a user experience that includes reminders that AI-generated content may be inaccurate. These reminders are present in any interface where AI-generated output is displayed. We strongly recommend that third-party developers incorporate these reminders in any user experience that they build themselves.

Users are reminded to be mindful of data privacy concerns and are encouraged to review the privacy policies associated with any models and data storage solutions interfacing with ReMe. NOTICE: The template implementation stores patient data locally on the machine where it runs. Lifelog data and user dialogue history are stored in a local SQLite database. The template code is only intended as a starting point, and Microsoft strongly encourages developers to adapt the storage and management methods to align with their specific use cases and security requirements. For example, developers may integrate encrypted databases, implement secure access controls, or utilize other secure storage mechanisms depending on the sensitivity of the data involved.

It is the user’s responsibility to ensure that the use of ReMe complies with relevant data protection regulations and organizational guidelines.

## CONTACT

We welcome feedback and collaboration from our audience. If you have suggestions, questions, or observe unexpected/offensive behavior in our technology, please contact us at wangzilong@microsoft.com.
